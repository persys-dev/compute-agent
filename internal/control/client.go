package control

import (
	"context"
	"crypto/tls"
	"crypto/x509"
	"fmt"
	"math"
	"net"
	"os"
	"strconv"
	"strings"
	"time"

	"github.com/persys/compute-agent/internal/config"
	"github.com/persys/compute-agent/internal/resources"
	"github.com/persys/compute-agent/internal/runtime"
	"github.com/persys/compute-agent/internal/workload"
	controlv1 "github.com/persys/compute-agent/pkg/control/v1"
	"github.com/persys/compute-agent/pkg/models"
	"github.com/shirou/gopsutil/v3/cpu"
	"github.com/shirou/gopsutil/v3/disk"
	"github.com/shirou/gopsutil/v3/mem"
	"github.com/sirupsen/logrus"
	"google.golang.org/grpc"
	"google.golang.org/grpc/credentials"
	"google.golang.org/grpc/credentials/insecure"
	"google.golang.org/protobuf/types/known/timestamppb"
)

const (
	defaultRPCTimeout       = 10 * time.Second
	defaultHeartbeatSeconds = 10
)

// Client manages scheduler control-plane communication.
type Client struct {
	cfg             *config.Config
	workloadManager *workload.Manager
	runtimeMgr      *runtime.Manager
	resourceMonitor *resources.Monitor
	logger          *logrus.Entry
}

func NewClient(cfg *config.Config, workloadManager *workload.Manager, runtimeMgr *runtime.Manager, resourceMonitor *resources.Monitor, logger *logrus.Logger) *Client {
	return &Client{
		cfg:             cfg,
		workloadManager: workloadManager,
		runtimeMgr:      runtimeMgr,
		resourceMonitor: resourceMonitor,
		logger:          logger.WithField("component", "scheduler-control-client"),
	}
}

// ApplyWorkload forwards workload intent to the scheduler control plane.
func (c *Client) ApplyWorkload(ctx context.Context, req *controlv1.ApplyWorkloadRequest) (*controlv1.ApplyWorkloadResponse, error) {
	conn, client, err := c.dial(ctx)
	if err != nil {
		return nil, err
	}
	defer conn.Close()

	rpcCtx, cancel := context.WithTimeout(ctx, defaultRPCTimeout)
	defer cancel()
	return client.ApplyWorkload(rpcCtx, req)
}

// DeleteWorkload forwards workload deletion intent to the scheduler control plane.
func (c *Client) DeleteWorkload(ctx context.Context, req *controlv1.DeleteWorkloadRequest) (*controlv1.DeleteWorkloadResponse, error) {
	conn, client, err := c.dial(ctx)
	if err != nil {
		return nil, err
	}
	defer conn.Close()

	rpcCtx, cancel := context.WithTimeout(ctx, defaultRPCTimeout)
	defer cancel()
	return client.DeleteWorkload(rpcCtx, req)
}

// RetryWorkload forwards retry intent to the scheduler control plane.
func (c *Client) RetryWorkload(ctx context.Context, req *controlv1.RetryWorkloadRequest) (*controlv1.RetryWorkloadResponse, error) {
	conn, client, err := c.dial(ctx)
	if err != nil {
		return nil, err
	}
	defer conn.Close()

	rpcCtx, cancel := context.WithTimeout(ctx, defaultRPCTimeout)
	defer cancel()
	return client.RetryWorkload(rpcCtx, req)
}

// Run starts registration/heartbeat loop and blocks until ctx is canceled.
func (c *Client) Run(ctx context.Context) {
	backoff := time.Second

	for {
		if ctx.Err() != nil {
			return
		}

		conn, client, err := c.dial(ctx)
		if err != nil {
			c.logger.WithError(err).Warn("Failed to connect to scheduler control endpoint")
			if !sleepWithContext(ctx, backoff) {
				return
			}
			backoff = nextBackoff(backoff)
			continue
		}

		regResp, err := c.register(ctx, client)
		if err != nil {
			c.logger.WithError(err).Warn("Node registration failed")
			_ = conn.Close()
			if !sleepWithContext(ctx, backoff) {
				return
			}
			backoff = nextBackoff(backoff)
			continue
		}
		if !regResp.GetAccepted() {
			c.logger.WithField("reason", regResp.GetReason()).Warn("Scheduler rejected node registration")
			_ = conn.Close()
			if !sleepWithContext(ctx, backoff) {
				return
			}
			backoff = nextBackoff(backoff)
			continue
		}

		backoff = time.Second
		interval := time.Duration(regResp.GetHeartbeatIntervalSeconds()) * time.Second
		if interval <= 0 {
			interval = defaultHeartbeatSeconds * time.Second
		}
		leaseExpiry := timestampToTime(regResp.GetLeaseExpiresAt())

		c.logger.WithFields(logrus.Fields{
			"heartbeat_interval": interval,
			"lease_expires_at":   leaseExpiry.Format(time.RFC3339),
		}).Info("Successfully registered node with scheduler")

		if _, err := c.heartbeat(ctx, client); err != nil {
			c.logger.WithError(err).Warn("Initial heartbeat failed after registration")
			_ = conn.Close()
			continue
		}

		ticker := time.NewTicker(interval)
		reconnect := false
		for !reconnect {
			select {
			case <-ctx.Done():
				ticker.Stop()
				_ = conn.Close()
				return
			case <-ticker.C:
				if !leaseExpiry.IsZero() && time.Now().After(leaseExpiry) {
					c.logger.Warn("Node lease expired; re-registering")
					reconnect = true
					continue
				}

				hbResp, err := c.heartbeat(ctx, client)
				if err != nil {
					c.logger.WithError(err).Warn("Heartbeat failed; reconnecting")
					reconnect = true
					continue
				}
				if hbResp.GetLeaseExpiresAt() != nil {
					leaseExpiry = timestampToTime(hbResp.GetLeaseExpiresAt())
				}
				if hbResp.GetDrainNode() {
					c.logger.Warn("Scheduler requested drain mode for this node")
				}
				if !hbResp.GetAcknowledged() {
					c.logger.Warn("Heartbeat was not acknowledged")
				}
			}
		}

		ticker.Stop()
		_ = conn.Close()
	}
}

func (c *Client) register(ctx context.Context, client controlv1.AgentControlClient) (*controlv1.RegisterNodeResponse, error) {
	capabilities, err := c.nodeCapabilities()
	if err != nil {
		return nil, err
	}

	rpcCtx, cancel := context.WithTimeout(ctx, defaultRPCTimeout)
	defer cancel()

	supported := c.supportedWorkloadTypes()
	c.logger.WithFields(logrus.Fields{
		"node_id":                  c.cfg.NodeID,
		"grpc_endpoint":            c.agentEndpoint(),
		"supported_workload_types": strings.Join(supported, ","),
	}).Info("Registering node with scheduler")

	return client.RegisterNode(rpcCtx, &controlv1.RegisterNodeRequest{
		NodeId:       c.cfg.NodeID,
		Capabilities: capabilities,
		Labels:       c.cfg.NodeLabels,
		AgentVersion: c.cfg.Version,
		GrpcEndpoint: c.agentEndpoint(),
		Timestamp:    timestamppb.Now(),
	})
}

func (c *Client) heartbeat(ctx context.Context, client controlv1.AgentControlClient) (*controlv1.HeartbeatResponse, error) {
	usage := c.nodeUsage()
	workloadStatuses := c.workloadStatuses(ctx)

	rpcCtx, cancel := context.WithTimeout(ctx, defaultRPCTimeout)
	defer cancel()

	return client.Heartbeat(rpcCtx, &controlv1.HeartbeatRequest{
		NodeId:           c.cfg.NodeID,
		Usage:            usage,
		WorkloadStatuses: workloadStatuses,
		Timestamp:        timestamppb.Now(),
	})
}

func (c *Client) dial(ctx context.Context) (*grpc.ClientConn, controlv1.AgentControlClient, error) {
	dialCtx, cancel := context.WithTimeout(ctx, defaultRPCTimeout)
	defer cancel()

	opts, err := c.dialOptions()
	if err != nil {
		return nil, nil, err
	}

	conn, err := grpc.DialContext(dialCtx, c.cfg.SchedulerAddr, opts...)
	if err != nil {
		return nil, nil, err
	}

	return conn, controlv1.NewAgentControlClient(conn), nil
}

func (c *Client) dialOptions() ([]grpc.DialOption, error) {
	if c.cfg.SchedulerInsecure || !c.cfg.SchedulerTLSEnabled {
		return []grpc.DialOption{grpc.WithTransportCredentials(insecure.NewCredentials())}, nil
	}

	cert, err := tls.LoadX509KeyPair(c.cfg.TLSCertPath, c.cfg.TLSKeyPath)
	if err != nil {
		return nil, fmt.Errorf("failed to load client cert/key: %w", err)
	}

	caCert, err := os.ReadFile(c.cfg.TLSCAPath)
	if err != nil {
		return nil, fmt.Errorf("failed to read CA certificate: %w", err)
	}

	pool := x509.NewCertPool()
	if !pool.AppendCertsFromPEM(caCert) {
		return nil, fmt.Errorf("failed to parse CA certificate")
	}

	tlsConfig := &tls.Config{
		MinVersion:   tls.VersionTLS12,
		RootCAs:      pool,
		Certificates: []tls.Certificate{cert},
	}

	return []grpc.DialOption{grpc.WithTransportCredentials(credentials.NewTLS(tlsConfig))}, nil
}

func (c *Client) nodeCapabilities() (*controlv1.NodeCapabilities, error) {
	cpuCount, err := cpu.Counts(true)
	if err != nil {
		return nil, fmt.Errorf("failed to get CPU count: %w", err)
	}
	memStats, err := mem.VirtualMemory()
	if err != nil {
		return nil, fmt.Errorf("failed to get memory stats: %w", err)
	}
	storagePools := []*controlv1.StoragePool{}
	if diskStats, err := disk.Usage("/"); err == nil {
		storagePools = append(storagePools, &controlv1.StoragePool{
			Name:    "root",
			Type:    "local",
			TotalGb: int64(diskStats.Total / 1024 / 1024 / 1024),
		})
	}

	return &controlv1.NodeCapabilities{
		CpuTotalMillicores:     int64(cpuCount * 1000),
		MemoryTotalMb:          int64(memStats.Total / 1024 / 1024),
		StoragePools:           storagePools,
		SupportedWorkloadTypes: c.supportedWorkloadTypes(),
	}, nil
}

func (c *Client) nodeUsage() *controlv1.NodeUsage {
	var cpuTotalMillicores int64
	if count, err := cpu.Counts(true); err == nil {
		cpuTotalMillicores = int64(count * 1000)
	}

	var cpuUsedMillicores int64
	var memoryUsedMB int64
	var diskUsedGB int64

	if c.resourceMonitor != nil {
		if util, err := c.resourceMonitor.GetUtilization([]string{"/"}); err == nil {
			cpuUsedMillicores = int64(math.Round(float64(cpuTotalMillicores) * (util.CPUPercent / 100.0)))
			if usage, ok := util.DiskPercent["/"]; ok {
				if diskStats, err := disk.Usage("/"); err == nil {
					diskUsedGB = int64(math.Round(float64(diskStats.Total/1024/1024/1024) * (usage / 100.0)))
				}
			}
		}
	}

	if vm, err := mem.VirtualMemory(); err == nil {
		memoryUsedMB = int64(vm.Used / 1024 / 1024)
	}
	if diskStats, err := disk.Usage("/"); err == nil && diskUsedGB == 0 {
		diskUsedGB = int64(diskStats.Used / 1024 / 1024 / 1024)
	}

	return &controlv1.NodeUsage{
		CpuAllocatedMillicores: 0,
		CpuUsedMillicores:      cpuUsedMillicores,
		MemoryAllocatedMb:      0,
		MemoryUsedMb:           memoryUsedMB,
		DiskAllocatedGb:        0,
		DiskUsedGb:             diskUsedGB,
	}
}

func (c *Client) workloadStatuses(ctx context.Context) []*controlv1.WorkloadStatus {
	if c.workloadManager == nil {
		return nil
	}
	statuses, err := c.workloadManager.ListWorkloads(ctx, nil)
	if err != nil {
		c.logger.WithError(err).Warn("Failed to list workload statuses for heartbeat")
		return nil
	}

	out := make([]*controlv1.WorkloadStatus, 0, len(statuses))
	for _, status := range statuses {
		if status == nil {
			continue
		}

		out = append(out, &controlv1.WorkloadStatus{
			WorkloadId:     status.ID,
			State:          normalizedState(string(status.ActualState)),
			FailureReason:  mapFailureReason(status),
			Message:        status.Message,
			LastTransition: timestamppb.New(nonZeroTime(status.UpdatedAt)),
		})
	}

	return out
}

func (c *Client) supportedWorkloadTypes() []string {
	types := make([]string, 0, 3)
	if c.runtimeMgr != nil && c.runtimeMgr.IsEnabled(models.WorkloadTypeContainer) {
		types = append(types, "container")
	}
	if c.runtimeMgr != nil && c.runtimeMgr.IsEnabled(models.WorkloadTypeCompose) {
		types = append(types, "compose")
	}
	if c.runtimeMgr != nil && c.runtimeMgr.IsEnabled(models.WorkloadTypeVM) {
		types = append(types, "vm")
	}

	// Fallback to config flags if runtime manager is unavailable.
	if c.runtimeMgr == nil {
		if c.cfg.DockerEnabled {
			types = append(types, "container")
		}
		if c.cfg.ComposeEnabled {
			types = append(types, "compose")
		}
		if c.cfg.VMEnabled {
			types = append(types, "vm")
		}
	}
	return types
}

func (c *Client) agentEndpoint() string {
	if c.cfg.AgentGRPCEndpoint != "" {
		return c.cfg.AgentGRPCEndpoint
	}

	host := c.cfg.GRPCAddr
	if isWildcardHost(host) || strings.EqualFold(host, "localhost") {
		if ip, err := outboundLocalIP(c.cfg.SchedulerAddr); err == nil && ip != "" {
			host = ip
		} else if ip, err := firstNonLoopbackIP(); err == nil && ip != "" {
			host = ip
		} else {
			host = "127.0.0.1"
		}
	}

	return net.JoinHostPort(host, strconv.Itoa(c.cfg.GRPCPort))
}

func normalizedState(in string) string {
	if in == "" {
		return "Unknown"
	}
	s := strings.ToLower(strings.TrimSpace(in))
	switch s {
	case "running":
		return "Running"
	case "stopped":
		return "Stopped"
	case "failed":
		return "Failed"
	case "pending":
		return "Pending"
	default:
		return "Unknown"
	}
}

func mapFailureReason(status *models.WorkloadStatus) controlv1.FailureReason {
	if status == nil {
		return controlv1.FailureReason_FAILURE_REASON_UNSPECIFIED
	}
	if strings.EqualFold(string(status.ActualState), "failed") {
		return controlv1.FailureReason_RUNTIME_ERROR
	}
	return controlv1.FailureReason_FAILURE_REASON_UNSPECIFIED
}

func nonZeroTime(t time.Time) time.Time {
	if t.IsZero() {
		return time.Now()
	}
	return t
}

func timestampToTime(ts *timestamppb.Timestamp) time.Time {
	if ts == nil {
		return time.Time{}
	}
	return ts.AsTime()
}

func sleepWithContext(ctx context.Context, d time.Duration) bool {
	t := time.NewTimer(d)
	defer t.Stop()

	select {
	case <-ctx.Done():
		return false
	case <-t.C:
		return true
	}
}

func nextBackoff(current time.Duration) time.Duration {
	next := current * 2
	if next > 30*time.Second {
		return 30 * time.Second
	}
	return next
}

func isWildcardHost(host string) bool {
	h := strings.TrimSpace(strings.ToLower(host))
	return h == "" || h == "0.0.0.0" || h == "::" || h == "[::]"
}

// outboundLocalIP picks the local source IP for traffic toward the scheduler.
// This avoids advertising hostnames that are not resolvable from scheduler.
func outboundLocalIP(schedulerAddr string) (string, error) {
	target := schedulerAddr
	if _, _, err := net.SplitHostPort(target); err != nil {
		// Handle bare host/IP values without explicit port.
		if strings.Contains(err.Error(), "missing port in address") {
			target = net.JoinHostPort(target, "80")
		} else {
			return "", err
		}
	}

	conn, err := net.Dial("udp", target)
	if err != nil {
		return "", err
	}
	defer conn.Close()

	udpAddr, ok := conn.LocalAddr().(*net.UDPAddr)
	if !ok || udpAddr.IP == nil {
		return "", fmt.Errorf("could not determine local UDP address")
	}
	if !udpAddr.IP.IsGlobalUnicast() {
		return "", fmt.Errorf("local IP is not global unicast: %s", udpAddr.IP.String())
	}
	return udpAddr.IP.String(), nil
}

func firstNonLoopbackIP() (string, error) {
	ifaces, err := net.Interfaces()
	if err != nil {
		return "", err
	}

	var ipv6Candidate string
	for _, iface := range ifaces {
		if iface.Flags&net.FlagUp == 0 || iface.Flags&net.FlagLoopback != 0 {
			continue
		}

		addrs, err := iface.Addrs()
		if err != nil {
			continue
		}
		for _, addr := range addrs {
			ipNet, ok := addr.(*net.IPNet)
			if !ok || ipNet.IP == nil || !ipNet.IP.IsGlobalUnicast() {
				continue
			}
			if v4 := ipNet.IP.To4(); v4 != nil {
				return v4.String(), nil
			}
			if ipv6Candidate == "" {
				ipv6Candidate = ipNet.IP.String()
			}
		}
	}

	if ipv6Candidate != "" {
		return ipv6Candidate, nil
	}
	return "", fmt.Errorf("no non-loopback IP address found")
}
